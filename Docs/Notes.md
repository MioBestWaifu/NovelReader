# Notes
This file is to register todo's, thoughts, plans, considerations and motivations. Explanations on how the things touched here work are elsewhere.

## REMINDERS

1. ~~Turn these txts into MD's~~
2. Register the Command API, noting the options available and expected from each command. 

## LARGE DATASETS AND STORAGE

Some of Maria's functionality is based on large datasets, notably translations, but others will probably follow. This presents two challenges.

1. How to store those things in the device? As of now, data generated by Maria is stored in CSV, but that can get very large if she is used for months or years. This, of course, makes the program size on the user's device larger than it needs to be. That is not a great problem now, since modern computers have a lot of storage. However, if Maria is ever ported to mobile or made into a web service, this will be a significant issue.

2. How to efficiently read them? Take the Japanese translation as an example. There are hundreds of thousands of base entries in the EDRDG database alone. Put together with the regular Japanese dictionary and a future conversion table and the number of entries may well be in the millions. This a problem even for modern computers, because reading from large text files is still a relatively slow operation. To avoid reading as much as possible, we are keeping as much data in memory as possible. As of writing, Maria can get to 700MB of RAM usage with the JMDICT alone. This does not seem much, given that the average target user will have more than 8GB of RAM, but is IS a lot for a supposedly lightweight program. Both of these considerations are obviously twice as bad for mobile devices. Android will not like a 1GB background service.

A possible solution to these two problems is to use a database. SQLite is the best candidate because it is very light and works on basically everything. Since the bulk of the code will be the same with or without it, this will be implemented in the future, if a code solution is not found. 

### 0.3 Update
It was ultimately decided against SQLite. The reason is that it is very slow to write a large database into it. While ideally the database will only be rewritten/updated rarely in production, in development this will happen multiple times and it is just unfeasible to await hours for every databse rewrite.  

The solution then, is to parse the EDRDG database into more compact files and a conversion table referencing those files from the word-keys. Thus, only a table saying "word x - file y entry z" would be in memory. Some caching would also help. Then the address is determined, file Y is loaded into memory, entry Z is retrieved and file Y is unloaded (cache scheme yet to be determined). For single-word searches, or searches where all keys are in the same file, this is perfectly feasible. Tests with JSON's with 1000 entries indicate that the reading and deserialization time is in the rage of 45-100ms, averaging around 60 ms (serialization was not optimized using small vairable property names). For multi-search queries this time would pile up and be a problem, but this is a problem for another version, and Maria is not suposed to translate whole sentences.  
  

As of now, JSON will be used for that purpose. However, it may be beneficial, both to storage and to reading times, to use a binary serialization protocol. This will be looked at in the future.
## TRANSLATION

As of now, only Japanese-to-English translations are needed, and so the translation implementation will not be abstracted, everything will be written only with jp-to-en in mind. However, the overall structure (namespace, entrypoints, etc) will be prepared for possible future expansion.

### MORPHOLOGICAL ANALYSIS AND DICTIONARY SIZES

To translate Japanese, the EDRDG database (mainly JMDICT) is used. However, the entries there are (unsurprisingly) in dictionary form. This means that to translate a flexioned verb, as an example, an unflexioned form of it must be determined. To that end a morphological analysis tool is the best fit. The chosen tool is a wrapper of MeCab, that uses IPADIC as its dictionary by default. However, this package bundles the dictionary into it, so it may bloat Maria.Services final size by up to 50mb. As of writing, this is unimportant. However, if size becomes a problem, another solution using the same analysis tool is to use its python wrapper (mecab-python3), because it does not bundle a dictionary, and so the app would only bloat if the user needs this function. This approach was not chosen at first because it is more convenient to use a C# wrapper. Another option, of course, is to build our own wrapper, but that is a lot of work for a small gain. Also, while using the current C# wrapper, it may be optimal do include unidic, since it is better-maintained and more complete. It is also, however, very large and so will only be available as an option. 

Also, the EDRDG database is itself very large, so in the end it is likely that the whole translation module bundled in the program will contain only code to interface with the database, and the database itself will be downloaded upon user input.

### JAPANESE SERIES NAMES

One of the files in the EDRDG database is a names dictionary. It contains names of people, places, and, I think, some art pieces like movies and anime. It is likely that this file will be incomplete. So, it will probably have to be expanded. It is relatively easy and feasible to expand it, especially with anime-related things, because the Japanese and English names of them are easily obtainable from MAL. Other categories I do not know, but it is probably easy to find the English name if we can determine that it is an art piece.