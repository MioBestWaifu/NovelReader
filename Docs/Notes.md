# Notes
This file is to register todo's, thoughts, plans, considerations and motivations. Explanations on how the things touched here work are elsewhere.

## REMINDERS

1. ~~Turn these txts into MD's~~
2. Register the Command API, noting the options available and expected from each command. 

## LARGE DATASETS AND STORAGE

Some of Maria's functionality is based on large datasets, notably translations, but others will probably follow. This presents two challenges.

1. How to store those things in the device? As of now, data generated by Maria is stored in CSV, but that can get very large if she is used for months or years. This, of course, makes the program size on the user's device larger than it needs to be. That is not a great problem now, since modern computers have a lot of storage. However, if Maria is ever ported to mobile or made into a web service, this will be a significant issue.

2. How to efficiently read them? Take the Japanese translation as an example. There are hundreds of thousands of base entries in the EDRDG database alone. Put together with the regular Japanese dictionary and a future conversion table and the number of entries may well be in the millions. This a problem even for modern computers, because reading from large text files is still a relatively slow operation. To avoid reading as much as possible, we are keeping as much data in memory as possible. As of writing, Maria can get to 700MB of RAM usage with the JMDICT alone. This does not seem much, given that the average target user will have more than 8GB of RAM, but is IS a lot for a supposedly lightweight program. Both of these considerations are obviously twice as bad for mobile devices. Android will not like a 1GB background service.

A possible solution to these two problems is to use a database. SQLite is the best candidate because it is very light and works on basically everything. Since the bulk of the code will be the same with or without it, this will be implemented in the future, if a code solution is not found. 

#### 0.3 Update
It was ultimately decided against SQLite. The reason is that it is very slow to write a large database into it. While ideally the database will only be rewritten/updated rarely in production, in development this will happen multiple times and it is just unfeasible to await hours for every databse rewrite.  

The solution then, is to parse the EDRDG database into more compact files and a conversion table referencing those files from the word-keys. Thus, only a table saying "word x - file y entry z" would be in memory. Some caching would also help. Then the address is determined, file Y is loaded into memory, entry Z is retrieved and file Y is unloaded (cache scheme yet to be determined). For single-word searches, or searches where all keys are in the same file, this is perfectly feasible. Tests with JSON's with 1000 entries indicate that the reading and deserialization time is in the rage of 45-100ms, averaging around 60 ms (serialization was not optimized using small vairable property names). For multi-search queries this time would pile up and be a problem, but this is a problem for another version, and Maria is not suposed to translate whole sentences.  
  

As of now, JSON will be used for that purpose. However, it may be beneficial, both to storage and to reading times, to use a binary serialization protocol. This will be looked at in the future.
## TRANSLATION

As of now, only Japanese-to-English translations are needed, and so the translation implementation will not be abstracted, everything will be written only with jp-to-en in mind. However, the overall structure (namespace, entrypoints, etc) will be prepared for possible future expansion.

### MORPHOLOGICAL ANALYSIS AND DICTIONARY SIZES

To translate Japanese, the EDRDG database (mainly JMDICT) is used. However, the entries there are (unsurprisingly) in dictionary form. This means that to translate a flexioned verb, as an example, an unflexioned form of it must be determined. To that end a morphological analysis tool is the best fit. The chosen tool is a wrapper of MeCab, that uses IPADIC as its dictionary by default. However, this package bundles the dictionary into it, so it may bloat Maria.Services final size by up to 50mb. As of writing, this is unimportant. However, if size becomes a problem, another solution using the same analysis tool is to use its python wrapper (mecab-python3), because it does not bundle a dictionary, and so the app would only bloat if the user needs this function. This approach was not chosen at first because it is more convenient to use a C# wrapper. Another option, of course, is to build our own wrapper, but that is a lot of work for a small gain. Also, while using the current C# wrapper, it may be optimal do include unidic, since it is better-maintained and more complete. It is also, however, very large and so will only be available as an option. 

Also, the EDRDG database is itself very large, so in the end it is likely that the whole translation module bundled in the program will contain only code to interface with the database, and the database itself will be downloaded upon user input.

#### 0.3 Update
Due to the insufficiency of the default dictionary, specially in handling continuous forms of godan verbs, the unidic will be the one always used. The user will have to accept this large download to translate japanese.

### JAPANESE SERIES NAMES

One of the files in the EDRDG database is a names dictionary. It contains names of people, places, and, I think, some art pieces like movies and anime. It is likely that this file will be incomplete. So, it will probably have to be expanded. It is relatively easy and feasible to expand it, especially with anime-related things, because the Japanese and English names of them are easily obtainable from MAL. Other categories I do not know, but it is probably easy to find the English name if we can determine that it is an art piece.

### EFFICIENCY
Due to the translator nature of looking up at big chunks data, sometimes multiple times for the same input, it has a tendency to take up a lot of memory and storage (look up the LARGE DATASETS AND STORAGE section above) and CPU in the form of response time. Because i am Maria's only user now, it doesn't really matter. But if she, or this particular service, is ever to be operationalized then memory usage and response time are of importance. As of 0.4 (May 13th, 2024), the response time seems good because in its intended use, the user will not be pumping loads of data, the blocks will at worst be a short sentence and the user will be paying attention to something else simultaneously. Memory usage is already to give worry. Maria is taking 160 MB of RAM with only the JMDict loaded, while operating no other service, and before word flexions are inserted into the conversion table.  

One way to reduce memory usage is by not keeping a conversion table at all and hash inputs to find their entry. Experiments were run on the following scheme: hashing the current about 275k keys using SHA-256, taking the first byte as a file number and the next 11 bits as a offset within that file. In those tests, the distribution of the first byte into the 0-255 range was found to be satisfactory, with most "files" close to the average of 1080 entries. The second byte is even better distributed. However, using the next 11 bits as offset generates a 20% chance of collision. This seems strange, and in full disclosure that algorithm was generated by Copilot, but the algo itself seems good. Anyway, if those results are close to correct, then using a 11 bit offset is little better than using just 8. So, we would use the second byte as an offset, and to deal with collisions the values in the file would be on array form, at which point it would be necessary to cycle trough them to find the match of the query. This approach might actually be more time-intensive than the current one, depending on the time do the hashing and looping, altough that is yet to be benchmarked.  

Another thing, this time to make storage more efficient, is that a two-step storage may be used because in the hashing scheme there would be no ConversionEntry to avoid data duplication. To avoid it in that case, the converison table could point to a file conatining only a pointer to another file containing the entries.

### CUSTOM READER AND AUTOMATIC TRANSLATION
To avoid having to build from scratch something already built and usable, i chose to integrate Maria with Kavita for the purposes of reading novels and the like. However, Kavita has a few annoying features that i am as of 0.4 (May 15th, 2024) unable to block with Extension. The worst is that if you select something in the epub reader, a menu drops in the top of the screen. This should theorically be stopabble, because it is a event registered in the DOM, Extension shoul be able to acess the DOM and remove it, but i still have not found a way to do so.  

A feature that might deal with it, and be handy anyways, is having the user not have to highlight anything at all and instead either press a button to translate the whole document or have it done automatically upon loading. This is feasible but requires some work on Services translation module. It could even allow to visually organize the translation page according to the translated document.  

Another possibility, altough it would be a truckload of work, is to build a book reader specifically for the purposes of translation. This would fix the aforementioned issue, yes, but primarily it would allow integration of a bunch of features that are nice, not strictly necessary and otherwise would be broken between various pages in Extension and Services. Such features could include a robot voice spelling the translated term or integrating viewing the original document, the english document (if available) and the translation page. As said, with the current perspective of Maria's use, this is a lot of work for not a lot of gain. However, it also presents a opportunity, because such a development could conceivably be separated from Maria and turned on its own application for commercial (or not) distribution to normal people (considering that anyone using Maria as envisioned including myself is not normal), something like The Japanese Novel Reader for Gaijin or a more marketable version of that. Either way, the translation module itself, which is the most complicated part, is mostly the same regardless, so development will remain unabated by this consideration. It is however, an interesting idea for the future.